---
title: 'Performance and Load Testing A New Application'
publishedAt: '2023-12-02'
summary: 'My process performance and load testing a new service during a migration'
tag: software-engineering
isPublished: false
---
Last year I worked on the replacement and migration of an application to a new one. This involved performance testing
the new service to ensure that it had at least the same or better level of performance and stability as the existing one.

Without this knowledge, we could have been replacing the old service with an even worse performing one!

Performance testing is defined a means to "[determine how a system performs in terms of responsiveness and stability under a [reasonable] workload](https://en.wikipedia.org/wiki/Software_performance_testing)".
A load test (or stress test) is just a variant of this at the upper limits by testing the system under extremely high loads.

It involves using a performance testing library to write tests that execute certain requests against
the application at a load level we specify.

Performance testing can be used for a variety of other reasons too, such as using it as a manual or automated sanity check to ensure new changes are stable.

This was a new experience for me which I was able to learn a lot about (with lots of guidance from my team) and so I wanted to document
the process I took.

## 1. Establishing the requirements
Now that we'd established _why_ we wanted to do these tests, next we needed to establish how they would work.

The must-have requirements were that it could:
- Be used to test against the service's different types of API protocols (e.g. JSON-RPC, gRPC) to get wide coverage
- Be run in an environment as similar to production as possible (e.g. consider the specs and resources allocated to the application and database)

The nice-to-have requirements of the performance tests were:
- To be able to spin up its own separate load testing environment so that it didn't impact an existing environment used by other people
- To make it simple to configure and run e.g. via the GitLab job UI instead of directly via command line

## 2. Establish metrics to capture
The primary metrics I wanted to measure aimed at performance were:
1. Throughput (Requests Per Second or RPS)
2. Latency (response time in percentiles e.g. P90, P99, P99.9)
3. Error rate

The secondary metrics I also measured aimed at gauging stability were:
1. Any incidental event throughput (e.g. Kafka) and downstream lag
2. Application memory usage
3. Database stability e.g. CPU utilisation, I/O, slow queries

## 3. Establish variables to test against
There were certain dimensions I tested against to address certain risks. That means I tested each test scenario multiple times
with these variables tuned differently but everything else remaining constant.

1. __Number of application pods running.__ The question to answer is - at what point does the database become the bottleneck as the application scales up? Testing against different configurations of this lets us know how the database would perform if the application is ever scaled down or up.
2. __Number of accounts.__ If there is any contention (e.g. database locking) how is performance affected by the number of accounts being operated on concurrently?

## 4. Establish test scenarios
This took a bit of experimentation to understand what the best approach was, but in retrospect here are some different scenarios
I would consider in the future:
- Testing different APIs with different responsibilities (e.g. a read API, write API)
- Testing different types of API protocols (e.g. does a JSON-RPC API perform differently compared to a REST or gRPC API?)
- Testing different requests which model real requests a client might make with varying complexities (e.g. a request that would result in at least 1 very simple code flow, and at least another that would result in a very complex code flow)
- Having test scenarios that deliberately invoke an unhappy path occasionally to see if the application can handle and recover from failure cases efficiently

## 5. Writing and executing the tests
In my case I was using the Python library [Locust](https://locust.io/). When executing the tests the main parameters I had to configure were:
1. Users: The maximum number of concurrent Locust users making requests. Each user would represent an individual executing thread in the API.
2. Spawn rate: The rate of users spawned per second. This is the 'ramp-up' rate where spawning stops when it reaches the max. number of users we specified above.
3. Run time: How long the tests run for.

Other performance testing libraries may have similar parameters.

## 6. Evaluating the results
There are several questions we can ask ourselves:
1. Do the results answer our original questions or hypotheses?
2. Do the results help us assess the risks we seeked to address?
3. Do the results make sense? In my case for example, one of my earlier tests showed throughput that was unusually high. It turned out that the way the database was configured in the environment I ran the tests in overinflated the application's performance.
4. Seek explanations for any differences in results across test scenarios.


## 7. Presenting the results
I would recommend not simply presenting the raw dataset. I found it was better to summarise the results for the given stakeholders including:
1. Summarising key insights or trends clearly. It's important to address the implications across changed variables and different test scenarios.
2. Highlighting any areas of concern
